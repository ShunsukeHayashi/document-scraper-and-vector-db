# ドキュメント抽出とベクトルデータベース構築

このプロジェクトは、指定されたルートURLからウェブページをスクレイピングし、関連するドキュメント情報を抽出してテキストファイルにまとめます。その後、テキストデータをベクトル化し、ベクトルデータベースを構築します。

## プロジェクトの目的

- ウェブページからドキュメントを自動的に抽出し、テキストファイルに保存します。
- テキストデータをベクトル化し、効率的な検索や類似度計算のためのベクトルデータベースを構築します。

## ディレクトリ構造

```
doc_scraping/
│
├── .env                   # 環境変数を設定するファイル
├── documents.txt          # スクレイピングされたドキュメントを保存するテキストファイル
├── vector_database.py     # ベクトルデータベースの構築と管理を行うスクリプト
├── run_all.py             # 全体のプロセスを実行するスクリプト
├── document_scraper.py    # ウェブページからドキュメントをスクレイピングするスクリプト
├── url_tree.txt           # スクレイピングしたURLのツリー構造を保存するファイル
├── Dockerfile             # Docker環境を構築するためのファイル
├── docker-compose.yml     # Docker Compose設定ファイル
├── requirements.txt       # プロジェクトの依存関係を記載したファイル
└── readme.md              # プロジェクトの説明と使用方法を記載したファイル
```

## ファイルの依存関係

- `document_scraper.py`:
  - 指定されたルートURLからドキュメントをスクレイピングし、`documents.txt`に保存します。
  - スクレイピングしたURLのツリー構造を`url_tree.txt`に保存します。

- `vector_database.py`:
  - `documents.txt`からテキストデータを読み込み、ドキュメントをチャンキングします。
  - チャンキングされたデータをベクトル化し、ベクトルデータベースを構築します。
  - ベクトルデータベースを`vector_database.index`として保存します。

- `run_all.py`:
  - `document_scraper.py`と`vector_database.py`を順に実行し、全体のプロセスを管理します。

## 使用方法

1. **ドキュメントのスクレイピング**
   - `document_scraper.py`を実行して、指定されたルートURLからドキュメントを抽出します。
   - 抽出されたドキュメントは`doc_scraping/documents.txt`に保存されます。

2. **ベクトルデータベースの構築**
   - `vector_database.py`を実行して、`documents.txt`からテキストデータを読み込み、ベクトル化します。
   - ベクトル化されたデータを使用して、ベクトルデータベースを構築します。
   - ベクトルデータベースは`vector_database.index`として保存されます。

3. **ベクトルデータベースの保存と読み込み**
   - `save_vector_database`関数を使用して、ベクトルデータベースをファイルに保存します。
   - `load_vector_database`関数を使用して、保存されたベクトルデータベースを読み込みます。

## 必要なライブラリ

- `openai`
- `pandas`
- `numpy`
- `faiss`
- `requests`
- `beautifulsoup4`
- `python-dotenv`

## 注意事項

- OpenAI APIキーを設定する必要があります。`.env`ファイルにAPIキーを設定してください。
- `openai`ライブラリのバージョンが1.0.0以上であることを確認してください。必要に応じて、`pip install openai --upgrade`を実行してライブラリをアップデートしてください。

## ライセンス

このプロジェクトはMITライセンスの下で公開されています。